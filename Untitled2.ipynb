{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ab52ba2-c50a-43ea-99cc-f80c55bb4241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Reshape, Dropout, GRU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paths\n",
    "CSV_PATH = r\"C:\\Users\\haswa.HASWANTH\\Downloads\\Data science project\\Licplatesrecognition_train.csv\"\n",
    "IMG_DIR = r\"C:\\Users\\haswa.HASWANTH\\Downloads\\Data science project\\Licplatesrecognition_train\\license_plates_recognition_train\"\n",
    "\n",
    "# Constants\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "MAX_TEXT_LENGTH = 10  # Adjust based on dataset\n",
    "BATCH_SIZE = 32\n",
    "arabic_charset = \"٠١٢٣٤٥٦٧٨٩أبتثجحخدذرزسشصضطظعغفقكلمنهوي\"\n",
    "\n",
    "# Mapping for Arabic numbers\n",
    "char_to_num = {\n",
    "    '٠': '0', '١': '1', '٢': '2', '٣': '3', '٤': '4', '٥': '5',\n",
    "    '٦': '6', '٧': '7', '٨': '8', '٩': '9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ec72678-15aa-4e3d-a81d-4c7bddaac77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "data = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Preprocess images and labels\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=\"grayscale\")\n",
    "    return img_to_array(img) / 255.0  # Normalize pixel values\n",
    "\n",
    "def encode_labels(label, charset):\n",
    "    return [charset.index(char) for char in label if char in charset]\n",
    "\n",
    "# Prepare dataset\n",
    "image_paths = [os.path.join(IMG_DIR, fname) for fname in data['img_id']]\n",
    "images = np.array([preprocess_image(path) for path in image_paths])\n",
    "labels = np.array([encode_labels(text, arabic_charset) for text in data['text']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "531f51bf-17f1-4c1a-bcc1-7f0005eef358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Custom data generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_images = self.images[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        padded_labels = tf.keras.preprocessing.sequence.pad_sequences(batch_labels, maxlen=MAX_TEXT_LENGTH, padding=\"post\")\n",
    "        return {\"input_image\": batch_images, \"input_label\": padded_labels}, np.zeros(len(batch_images))\n",
    "\n",
    "train_generator = DataGenerator(X_train, y_train, BATCH_SIZE)\n",
    "val_generator = DataGenerator(X_val, y_val, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92cdf2d6-7f5b-447d-9894-8595ccff21c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CRNN_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 50, 200, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 50, 200, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 25, 100, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 25, 100, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPoolin  (None, 12, 50, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 600, 64)           0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 600, 128)          74496     \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 600, 128)          99072     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 600, 39)           5031      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197415 (771.15 KB)\n",
      "Trainable params: 197415 (771.15 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define CRNN Model\n",
    "def build_crnn_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape, name=\"input_image\")\n",
    "    labels = Input(shape=(MAX_TEXT_LENGTH,), name=\"input_label\")\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Flatten and Reshape\n",
    "    x = Reshape((-1, 64))(x)  # Shape: (None, timesteps, features)\n",
    "\n",
    "    # Recurrent layers\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x, name=\"CRNN_Model\")\n",
    "\n",
    "# Build the model\n",
    "crnn_model = build_crnn_model((IMG_HEIGHT, IMG_WIDTH, 1), len(arabic_charset) + 1)  # +1 for CTC blank token\n",
    "crnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a220f43-7671-4fb9-b280-c8762b51ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CTC Loss with correct input_length and label_length\n",
    "def ctc_loss_function(y_true, y_pred):\n",
    "    # Set the correct input length (width of the image after pooling)\n",
    "    input_length = tf.ones((BATCH_SIZE, 1)) * (IMG_WIDTH // 4)  # Adjust based on your model's architecture (pooling)\n",
    "    \n",
    "    # Set the correct label length (maximum text length)\n",
    "    label_length = tf.ones((BATCH_SIZE, 1)) * MAX_TEXT_LENGTH\n",
    "    \n",
    "    # Return CTC loss\n",
    "    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02b3e5bc-2798-4079-8cbe-e250b246e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Data Generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, images, labels, batch_size):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_images = self.images[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        # Pad labels to the max text length\n",
    "        padded_labels = tf.keras.preprocessing.sequence.pad_sequences(batch_labels, maxlen=MAX_TEXT_LENGTH, padding=\"post\")\n",
    "        \n",
    "        # Return images and padded labels as a dictionary (CTC expects this format)\n",
    "        return {\"input_image\": batch_images, \"input_label\": padded_labels}, np.zeros(len(batch_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0f81f0a-260c-41b9-b44d-8b89725f2bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CRNN_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_image (InputLayer)    [(None, 50, 200, 1)]      0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 50, 200, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d_9 (MaxPoolin  (None, 25, 100, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 25, 100, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPooli  (None, 12, 50, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 600, 64)           0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 600, 128)          74496     \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 600, 128)          99072     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 600, 39)           5031      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 197415 (771.15 KB)\n",
      "Trainable params: 197415 (771.15 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_crnn_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape, name=\"input_image\")\n",
    "    labels = Input(shape=(MAX_TEXT_LENGTH,), name=\"input_label\")\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Flatten and Reshape\n",
    "    x = Reshape((-1, 64))(x)  # Shape: (None, timesteps, features)\n",
    "\n",
    "    # Recurrent layers\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "\n",
    "    # Fully connected layer\n",
    "    x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x, name=\"CRNN_Model\")\n",
    "\n",
    "# Build the model\n",
    "crnn_model = build_crnn_model((IMG_HEIGHT, IMG_WIDTH, 1), len(arabic_charset) + 1)  # +1 for CTC blank token\n",
    "crnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69ccc6d9-3e2e-4e9c-8b18-3bd226143977",
   "metadata": {},
   "outputs": [],
   "source": [
    "crnn_model.compile(optimizer=Adam(), loss=ctc_loss_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99001c86-4d73-4fdb-972d-d19fe4083e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Temp\\ipykernel_5784\\1042911898.py\", line 10, in ctc_loss_function  *\n        return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 7161, in ctc_batch_cost\n        ctc_label_dense_to_sparse(y_true, label_length), tf.int32\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 7102, in ctc_label_dense_to_sparse\n        max_num_labels_tns = tf.stack([label_shape[1]])\n\n    ValueError: slice index 1 of dimension 0 out of bounds. for '{{node ctc_loss_function/strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](ctc_loss_function/Shape, ctc_loss_function/strided_slice_1/stack, ctc_loss_function/strided_slice_1/stack_1, ctc_loss_function/strided_slice_1/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\HASWA~1.HAS\\AppData\\Local\\Temp\\__autograph_generated_filetu7ztdg8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\HASWA~1.HAS\\AppData\\Local\\Temp\\__autograph_generated_file6ezb7rx3.py:14\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__ctc_loss_function\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mctc_batch_cost, (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred), ag__\u001b[38;5;241m.\u001b[39mld(input_length), ag__\u001b[38;5;241m.\u001b[39mld(label_length)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Temp\\ipykernel_5784\\1042911898.py\", line 10, in ctc_loss_function  *\n        return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 7161, in ctc_batch_cost\n        ctc_label_dense_to_sparse(y_true, label_length), tf.int32\n    File \"C:\\Users\\haswa.HASWANTH\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py\", line 7102, in ctc_label_dense_to_sparse\n        max_num_labels_tns = tf.stack([label_shape[1]])\n\n    ValueError: slice index 1 of dimension 0 out of bounds. for '{{node ctc_loss_function/strided_slice_1}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](ctc_loss_function/Shape, ctc_loss_function/strided_slice_1/stack, ctc_loss_function/strided_slice_1/stack_1, ctc_loss_function/strided_slice_1/stack_2)' with input shapes: [1], [1], [1], [1] and with computed input tensors: input[1] = <1>, input[2] = <2>, input[3] = <1>.\n"
     ]
    }
   ],
   "source": [
    "history = crnn_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(val_generator)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3ac00-d6c6-4c40-9578-d62848ed2f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
